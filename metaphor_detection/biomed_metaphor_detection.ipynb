{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lwachowiak/Multilingual-Metaphor-Detection/blob/main/Metaphor_Detection_(Tokenlevel).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yMTmZptEkHC"
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JamBNJ_frr6T"
   },
   "outputs": [],
   "source": [
    "# !pip install pynvml\n",
    "# !pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m9fYtB3_FHuK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\melou\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#torch and tranformers for model and training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sentencepiece\n",
    "\n",
    "#sklearn for evaluation\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#utilities\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pickle         # for saving data structures\n",
    "from pynvml import *  # for checking gpu memory\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyekDMZ28gBc",
    "outputId": "a730cc4e-ab6a-48dc-cdf5-4246ced98b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# connect to GPU\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print('Connected to GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RPZ14sYHHUm"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU7NMPaDvbWt"
   },
   "source": [
    "**Functions for preprocessing and creating of Training Data**\n",
    "\n",
    "Originally I used *xlm-roberta-base* as model. Now, there are slightly stronger models available in the same parameter range, for example *microsoft/mdeberta-v3-base*\n",
    "\n",
    "\n",
    "You can try:\n",
    "\n",
    "\n",
    "*   model_name=\"xlm-roberta-base\"\n",
    "*   model_name=\"xlm-roberta-large\"\n",
    "*   model_name=\"microsoft/mdeberta-v3-base\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BVaK3boLMOWT"
   },
   "outputs": [],
   "source": [
    "model_name=\"allenai/biomed_roberta_base\"\n",
    "random_validation=True\n",
    "random_state=1\n",
    "val_percentage=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoHbshOctCeU"
   },
   "source": [
    "Upload files to Google Drive or link to your computer's folder if running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nSQa-1aBnk_g"
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "test_data=pd.read_csv(\"VUA_test_all.csv\", engine=\"python\")\n",
    "train_data=pd.read_csv(\"VUA_train.csv\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mplkMdtDnlHT"
   },
   "outputs": [],
   "source": [
    "def format_for_TokenClf(df) -> list:\n",
    "  data_list = []\n",
    "  sentence=[]\n",
    "  labels=[]\n",
    "  for index, row in df.iterrows():\n",
    "    if row[\"id\"][-2:]==\"_1\" and index!=0:\n",
    "      data_list.append((sentence, labels))\n",
    "      sentence=[]\n",
    "      labels=[]\n",
    "    if row[\"label\"]==1:\n",
    "      label=\"m\"\n",
    "    else:\n",
    "      label=\"l\"\n",
    "    sentence.append(str(row[\"word\"]))\n",
    "    labels.append(label)\n",
    "    if index==len(df)-1:\n",
    "      data_list.append((sentence, labels))\n",
    "\n",
    "  return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuqOpet9nlPe",
    "outputId": "601eabdf-a650-4373-b890-d5c9c27dd849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['The', 'Labour', 'Party', 'Conference', ':', 'Policy', 'review', 'throws', 'a', 'spanner', 'in', 'the', 'Whitehall', 'machinery'], ['l', 'l', 'l', 'l', 'l', 'l', 'l', 'm', 'l', 'm', 'm', 'l', 'l', 'm'])\n"
     ]
    }
   ],
   "source": [
    "test_data=format_for_TokenClf(test_data)\n",
    "train_data=format_for_TokenClf(train_data)\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3k9l6nbFcpUB"
   },
   "outputs": [],
   "source": [
    "train_data, val_data= train_test_split(train_data, shuffle=random_validation, test_size=val_percentage, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4tXvcFCda7t",
    "outputId": "a8ca571e-576f-49b3-fcb4-54ad7a979be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences Train:  10898\n",
      "Sentences Val:  1211\n",
      "Sentences Test:  4080\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences Train: \", len(train_data))\n",
    "print(\"Sentences Val: \", len(val_data))\n",
    "print(\"Sentences Test: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lKjeYB3ntwBn"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "train_tags=[tup[1] for tup in train_data]\n",
    "train_texts=[tup[0] for tup in train_data]\n",
    "\n",
    "#val\n",
    "val_tags=[tup[1] for tup in val_data]\n",
    "val_texts=[tup[0] for tup in val_data]\n",
    "\n",
    "#test\n",
    "test_tags=[tup[1] for tup in test_data]\n",
    "test_texts=[tup[0] for tup in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "S6cd6VWqxNQ9"
   },
   "outputs": [],
   "source": [
    "for text in train_tags:\n",
    "  if not isinstance(text, list):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hH9lPm3ivXa5",
    "outputId": "23f9ffa4-3b5c-4f8a-8ffc-88d2c7f20061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Labour', 'Party', 'Conference', ':', 'Policy', 'review', 'throws', 'a', 'spanner', 'in', 'the', 'Whitehall', 'machinery']\n",
      "['l', 'l', 'l', 'l', 'l', 'l', 'l', 'm', 'l', 'm', 'm', 'l', 'l', 'm']\n"
     ]
    }
   ],
   "source": [
    "print(test_texts[0])\n",
    "print(test_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVxAsANXfpDv"
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "_ieMHql0gobX"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "XYftDnmguJMr"
   },
   "outputs": [],
   "source": [
    "label_list=[\"l\", \"m\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels=len(label_list)\n",
    "\n",
    "def tokenize_and_align_labels(texts, tags):\n",
    "  tokenized_inputs = tokenizer(\n",
    "      texts,\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "      is_split_into_words=True,\n",
    "  )\n",
    "  labels = []\n",
    "  for i, label in enumerate(tags):\n",
    "      word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "      previous_word_idx = None\n",
    "      label_ids = []\n",
    "      for word_idx in word_ids:\n",
    "          # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "          # ignored in the loss function.\n",
    "          if word_idx is None:\n",
    "              label_ids.append(-100)\n",
    "          # We set the label for the first token of each word.\n",
    "          elif word_idx != previous_word_idx:\n",
    "              label_ids.append(label_to_id[label[word_idx]])\n",
    "          # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "          # the label_all_tokens flag.\n",
    "          else:\n",
    "              label_ids.append(-100)\n",
    "          previous_word_idx = word_idx\n",
    "\n",
    "      labels.append(label_ids)\n",
    "  tokenized_inputs[\"labels\"] = labels\n",
    "  return tokenized_inputs\n",
    "\n",
    "test_input_and_labels = tokenize_and_align_labels(test_texts, test_tags)\n",
    "\n",
    "val_input_and_labels = tokenize_and_align_labels(val_texts, val_tags)\n",
    "\n",
    "train_input_and_labels = tokenize_and_align_labels(train_texts, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "6lcPXbZ22yWG"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "test_dataset = OurDataset(test_input_and_labels, test_input_and_labels[\"labels\"])\n",
    "\n",
    "train_dataset = OurDataset(train_input_and_labels, train_input_and_labels[\"labels\"])\n",
    "\n",
    "val_dataset = OurDataset(val_input_and_labels, val_input_and_labels[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50I3MZhnod-Y",
    "outputId": "7956cea0-d7ee-4114-e257-d6be57c97839"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,    20,  4165,  1643,  2815,  4832,  6275,  1551,  6989,    10,\n",
       "          8968,  1396,    11,     5,   735, 12023, 13922,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([-100,    0,    0,    0,    0,    0,    0,    0,    1,    0,    1, -100,\n",
       "            1,    0,    0, -100,    1, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9miQ8_HxKqGB"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "lE8JxaF5T9Yd"
   },
   "outputs": [],
   "source": [
    "# how the validation and test scores are computed\n",
    "\n",
    "def compute_metrics(eval_preds) -> dict:\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [val for sublist in true_labels for val in sublist]\n",
    "    true_predictions = [val for sublist in true_predictions for val in sublist]\n",
    "\n",
    "    print(classification_report(true_labels, true_predictions))#, target_names=target_names))\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_predictions, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcDLjK-i4-Y8",
    "outputId": "8a87ee0f-eea2-47ea-8126-8466c7580fbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./MetaphorExtraction/results',          # output directory\n",
    "    num_train_epochs=8,                                 # total # of training epochs\n",
    "    per_device_train_batch_size=8,                      # batch size per device during training\n",
    "    per_device_eval_batch_size=16,                      # batch size for evaluation\n",
    "    warmup_steps=0,                                     # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0,                                     # strength of weight decay\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./MetaphorExtraction/logs',            # directory for storing logs\n",
    "    evaluation_strategy= \"epoch\",                       # steps or epochs\n",
    "    save_strategy = \"epoch\",\n",
    "    # eval_steps=500,\n",
    "    # save_total_limit=0,\n",
    "    load_best_model_at_end=True,                        #loads the model with the best evaluation score\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "329fd8dd21b9494d9b8aacbd0cdccc9d",
      "120fa8b9822f4deb850b2a0782f606ea",
      "46e495b9227f4231ae7a9904a5eb56ab",
      "c3778f68ba1946bbb863d81ca1b5f364",
      "53f9f3d4b6ce463ab6bb1fcc29a0beee",
      "6c64dee80d954a25b9b0ae6edf6ccd36",
      "f4b13428f9f84aaca22f0a3ea1c3e360",
      "9599a6df3a5245bd94392a9517a6da1e",
      "600388f69599490b8c431fa3debb469e",
      "0343acd622bd404491af724d295357b2",
      "d88e26a4ec4a4c3791855ec906a05788"
     ]
    },
    "id": "yJf_Rnyf26el",
    "outputId": "f4ef5915-ea4a-4e79-9aa1-bc21f4c62d9c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbad2f3397e3419a988df1a0952f50d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/656M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\melou\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LPAH8QvBezG",
    "outputId": "67f1ad32-77d9-4d47-e959-268832b4be02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc97a5ff7ff4e3aac664a71800f40a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/656M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 8585740288\n",
      "free     : 8334082048\n",
      "used     : 251658240\n"
     ]
    }
   ],
   "source": [
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "wEg8krxu-MY4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_6844\\120607166.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# initialize huggingface trainer\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nZIimqAK28qS",
    "outputId": "ccbdd4df-3925-4879-c85f-5b9b28fe516b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10904' max='10904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10904/10904 26:05, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.129684</td>\n",
       "      <td>0.947333</td>\n",
       "      <td>0.950027</td>\n",
       "      <td>0.947733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>0.137604</td>\n",
       "      <td>0.949646</td>\n",
       "      <td>0.952062</td>\n",
       "      <td>0.950043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>0.142460</td>\n",
       "      <td>0.954476</td>\n",
       "      <td>0.956020</td>\n",
       "      <td>0.954979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.175513</td>\n",
       "      <td>0.953391</td>\n",
       "      <td>0.955525</td>\n",
       "      <td>0.953317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.196196</td>\n",
       "      <td>0.955333</td>\n",
       "      <td>0.956075</td>\n",
       "      <td>0.955660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.237231</td>\n",
       "      <td>0.954808</td>\n",
       "      <td>0.956075</td>\n",
       "      <td>0.955285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.262172</td>\n",
       "      <td>0.955082</td>\n",
       "      <td>0.956240</td>\n",
       "      <td>0.955534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.284935</td>\n",
       "      <td>0.955016</td>\n",
       "      <td>0.956460</td>\n",
       "      <td>0.955507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.96      0.98      0.97     16254\n",
      "           m       0.83      0.67      0.74      1936\n",
      "\n",
      "    accuracy                           0.95     18190\n",
      "   macro avg       0.89      0.83      0.86     18190\n",
      "weighted avg       0.95      0.95      0.95     18190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.96      0.98      0.97     16254\n",
      "           m       0.83      0.69      0.75      1936\n",
      "\n",
      "    accuracy                           0.95     18190\n",
      "   macro avg       0.90      0.84      0.86     18190\n",
      "weighted avg       0.95      0.95      0.95     18190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.97      0.98      0.98     16254\n",
      "           m       0.83      0.74      0.78      1936\n",
      "\n",
      "    accuracy                           0.96     18190\n",
      "   macro avg       0.90      0.86      0.88     18190\n",
      "weighted avg       0.95      0.96      0.95     18190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.96      0.99      0.98     16254\n",
      "           m       0.86      0.69      0.77      1936\n",
      "\n",
      "    accuracy                           0.96     18190\n",
      "   macro avg       0.91      0.84      0.87     18190\n",
      "weighted avg       0.95      0.96      0.95     18190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.97      0.98      0.98     16254\n",
      "           m       0.81      0.77      0.79      1936\n",
      "\n",
      "    accuracy                           0.96     18190\n",
      "   macro avg       0.89      0.88      0.88     18190\n",
      "weighted avg       0.96      0.96      0.96     18190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.97      0.98      0.98     16254\n",
      "           m       0.82      0.75      0.79      1936\n",
      "\n",
      "    accuracy                           0.96     18190\n",
      "   macro avg       0.89      0.87      0.88     18190\n",
      "weighted avg       0.95      0.96      0.96     18190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.97      0.98      0.98     16254\n",
      "           m       0.82      0.76      0.79      1936\n",
      "\n",
      "    accuracy                           0.96     18190\n",
      "   macro avg       0.89      0.87      0.88     18190\n",
      "weighted avg       0.96      0.96      0.96     18190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.97      0.98      0.98     16254\n",
      "           m       0.83      0.75      0.79      1936\n",
      "\n",
      "    accuracy                           0.96     18190\n",
      "   macro avg       0.90      0.86      0.88     18190\n",
      "weighted avg       0.96      0.96      0.96     18190\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10904, training_loss=0.05897337867333796, metrics={'train_runtime': 1567.1173, 'train_samples_per_second': 55.633, 'train_steps_per_second': 6.958, 'total_flos': 5517248356386048.0, 'train_loss': 0.05897337867333796, 'epoch': 8.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "xRZMADdvhNrw",
    "outputId": "6fbcea4d-97f6-43ab-a74f-56d29a9411eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           l       0.97      0.98      0.97     51540\n",
      "           m       0.80      0.75      0.78      6819\n",
      "\n",
      "    accuracy                           0.95     58359\n",
      "   macro avg       0.89      0.87      0.87     58359\n",
      "weighted avg       0.95      0.95      0.95     58359\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.20723897218704224,\n",
       " 'eval_precision': 0.948598086741333,\n",
       " 'eval_recall': 0.9497592487876763,\n",
       " 'eval_f1': 0.9490767337187377,\n",
       " 'eval_runtime': 16.8959,\n",
       " 'eval_samples_per_second': 241.479,\n",
       " 'eval_steps_per_second': 15.092,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on the test set\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovbaJVNyzw_O",
    "outputId": "b3e2ca13-2444-47f5-ff89-1309a6f9f8f9"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "trainer.save_model(\"./saved-models/metaphor_extraction_\"+str(date.today())+\"_randVal-\"+str(random_validation)+\"_\"+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_dYhX6t2FsM"
   },
   "source": [
    "# Using the Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "qnb4ruMEtso9"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "9fmqFqEDzEid"
   },
   "outputs": [],
   "source": [
    "label_list= ['literal',\"metaphoric\"]\n",
    "label_dict_relations={ i : l for i, l in enumerate(label_list) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "fjvnF0nIumQg"
   },
   "outputs": [],
   "source": [
    "PATH = \"./saved-models/metaphor_extraction_2025-04-15_randVal-True_allenai/biomed_roberta_base\"\n",
    "model_metaphor_detection = AutoModelForTokenClassification.from_pretrained(PATH, id2label=label_dict_relations)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "7kEZyuAbudIG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipeline_metaphors=pipeline(\"ner\", model=model_metaphor_detection, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FaHMbInvvDex",
    "outputId": "936859c6-98fa-44b9-c106-ea6a52c22868"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'literal',\n",
       "  'score': 0.9976692,\n",
       "  'word': 'Our love is',\n",
       "  'start': 0,\n",
       "  'end': 11},\n",
       " {'entity_group': 'metaphoric',\n",
       "  'score': 0.99968034,\n",
       "  'word': ' at crossroads',\n",
       "  'start': 12,\n",
       "  'end': 25},\n",
       " {'entity_group': 'literal',\n",
       "  'score': 0.99888617,\n",
       "  'word': ' and the company is',\n",
       "  'start': 26,\n",
       "  'end': 44},\n",
       " {'entity_group': 'metaphoric',\n",
       "  'score': 0.9857975,\n",
       "  'word': ' going into hibernation',\n",
       "  'start': 45,\n",
       "  'end': 67},\n",
       " {'entity_group': 'literal',\n",
       "  'score': 0.873021,\n",
       "  'word': '. The bear is sleeping well.',\n",
       "  'start': 67,\n",
       "  'end': 95}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_metaphors(\"Our love is at crossroads and the company is going into hibernation. The bear is sleeping well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMbmi2oAtwX+8RGMSlXp+yh",
   "include_colab_link": true,
   "mount_file_id": "1HBW3MvJrdK50MJxR3ZKbEgHAoiQdUp27",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0343acd622bd404491af724d295357b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "120fa8b9822f4deb850b2a0782f606ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c64dee80d954a25b9b0ae6edf6ccd36",
      "placeholder": "​",
      "style": "IPY_MODEL_f4b13428f9f84aaca22f0a3ea1c3e360",
      "value": "model.safetensors: 100%"
     }
    },
    "329fd8dd21b9494d9b8aacbd0cdccc9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_120fa8b9822f4deb850b2a0782f606ea",
       "IPY_MODEL_46e495b9227f4231ae7a9904a5eb56ab",
       "IPY_MODEL_c3778f68ba1946bbb863d81ca1b5f364"
      ],
      "layout": "IPY_MODEL_53f9f3d4b6ce463ab6bb1fcc29a0beee"
     }
    },
    "46e495b9227f4231ae7a9904a5eb56ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9599a6df3a5245bd94392a9517a6da1e",
      "max": 1115567652,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_600388f69599490b8c431fa3debb469e",
      "value": 1115567652
     }
    },
    "53f9f3d4b6ce463ab6bb1fcc29a0beee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "600388f69599490b8c431fa3debb469e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c64dee80d954a25b9b0ae6edf6ccd36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9599a6df3a5245bd94392a9517a6da1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3778f68ba1946bbb863d81ca1b5f364": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0343acd622bd404491af724d295357b2",
      "placeholder": "​",
      "style": "IPY_MODEL_d88e26a4ec4a4c3791855ec906a05788",
      "value": " 1.12G/1.12G [00:07&lt;00:00, 110MB/s]"
     }
    },
    "d88e26a4ec4a4c3791855ec906a05788": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4b13428f9f84aaca22f0a3ea1c3e360": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
