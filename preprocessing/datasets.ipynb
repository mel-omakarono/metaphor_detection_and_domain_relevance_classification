{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1426556a-89b6-4c7e-8763-c0026ebe2667",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f6f87-5dad-43ac-a310-e2007cd5b011",
   "metadata": {},
   "source": [
    "`Melina Paxinou`\n",
    "\n",
    "MA Linguistics, Text Mining - Vrije Universiteit Amsterdam\n",
    "\n",
    "June 27, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81d7de8-e9b4-42e5-89bc-ca7ea2199fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from openpyxl.utils.exceptions import IllegalCharacterError\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizerFast\n",
    "import random\n",
    "import os\n",
    "from rapidfuzz.fuzz import ratio\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import krippendorff\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from spacy.tokens import Doc\n",
    "import json\n",
    "import ast\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bef3836-5e6a-4311-962a-85289d02c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "polywords = [\n",
    "    \"a bit\", \"a capella\", \"a fortiori\", \"a good deal\", \"a great deal\",\n",
    "    \"a heck of a lot\", \"a hell of a lot\", \"à la\", \"à la carte\", \"à la mode\",\n",
    "    \"à la provençale\", \"a little\", \"a little bit\", \"a lot\", \"a posteriori\",\n",
    "    \"a priori\", \"a propos\", \"ab initio\", \"according as\", \"according to\",\n",
    "    \"ad astra\", \"ad hoc\", \"ad hominem\", \"ad infinitum\", \"ad lib\", \"ad libs\",\n",
    "    \"ad nauseam\", \"ad valorem\", \"adjacent to\", \"affaire d'honneur\",\n",
    "    \"affaire de coeur\", \"affaire du coeur\", \"affaires d'honneur\",\n",
    "    \"affaires de coeur\", \"agent provocateur\", \"agents provocateurs\",\n",
    "    \"agnus dei\", \"ahead of\", \"aide de camp\", \"aide memoire\", \"aides de camp\",\n",
    "    \"al dente\", \"al fresco\", \"all at once\", \"all but\", \"all of a sudden\",\n",
    "    \"all right\", \"all the same\", \"alla breve\", \"alma mater\", \"alma maters\",\n",
    "    \"along with\", \"alter ego\", \"alter egos\", \"an awful lot\", \"an' all\",\n",
    "    \"ancien régime\", \"and so forth\", \"and so on\", \"anno dom\", \"anno domini\",\n",
    "    \"annus horribilis\", \"annus mirabilis\", \"ante meridiem\", \"any longer\",\n",
    "    \"anything but\", \"apart from\", \"aqua vitae\", \"art deco\", \"art nouveau\",\n",
    "    \"as against\", \"as between\", \"as for\", \"as from\", \"as if\", \"as it were\",\n",
    "    \"as long as\", \"as of\", \"as opposed to\", \"as regards\", \"as soon as\",\n",
    "    \"as though\", \"as to\", \"as usual\", \"as well as\", \"as yet\", \"aside from\",\n",
    "    \"asti spumante\", \"at all\", \"at best\", \"at first\", \"at large\", \"at last\", \"at least\", \n",
    "    \"at length\", \"at long last\", \"at long length\", \"at most\", \"at once\", \"at present\", \n",
    "    \"at random\", \"at worst\", \"au contraire\", \"au fait\", \"au naturel\", \"au pair\", \"au pairs\", \n",
    "    \"au revoir\", \"auf Wiedersehen\", \"aurora australis\", \"aurora borealis\", \"avant garde\", \n",
    "    \"away from\", \"bar mitzvah\", \"bar mitzvahs\", \"basso continuo\", \"basso profundo\", \"beau monde\", \n",
    "    \"beaujolais nouveau\", \"because of\", \"belles lettres\", \"bête noir\", \"bête noire\", \"bêtes noires\", \n",
    "    \"billet doux\", \"billets doux\", \"bon appetit\", \"bon jour\", \"bon mot\", \"bon soir\", \"bon vivant\", \n",
    "    \"bon viveur\", \"bon voyage\", \"bona fide\", \"bona fides\", \"bons mots\", \"bons vivants\", \"bons viveurs\", \n",
    "    \"bouquet garni\", \"brand new\", \"bric à brac\", \"but for\", \"by and by\", \"by and large\", \"by far\", \n",
    "    \"by far and away\", \"by means of\", \"by no means\", \"by now\", \"by reason of\", \"by the by\", \"by way of\", \n",
    "    \"café au lait\", \"camera obscura\", \"camera obscuras\", \"carte blanche\", \"casus belli\", \"casus omissus\", \n",
    "    \"cause célèbre\", \"causes célèbres\", \"caveat emptor\", \"ceteris paribus\", \"chaise longue\", \n",
    "    \"chaise longues\", \"chaises longues\", \"chargé d'affaires\", \"chargés d'affaires\", \"check outs\", \n",
    "    \"chef d'oeuvre\", \"chez moi\", \"chez nous\", \"chez vous\", \"chilli con carne\", \"chop suey\", \"chow mein\", \n",
    "    \"clamp down\", \"close to\", \"compos mentis\", \"con brio\", \"con fuoco\", \"con moto\", \"considering that\", \n",
    "    \"contrary to\", \"cordon bleu\", \"cordon sanitaire\", \"corps de ballet\", \"corpus delicti\", \"corpus juris\", \n",
    "    \"coup d'état\", \"coup de foudre\", \"coup de grâce\", \"coup de théâtre\", \"coups d'état\", \"coups de grâce\", \n",
    "    \"coups de théâtre\", \"crème brulée\", \"crème de la crème\", \"crème de menthe\", \"crème fraîche\", \n",
    "    \"cri de coeur\", \"crime passionel\", \"crimes passionels\", \"cris de coeur\", \"croix de guerre\", \"cul de sac\", \n",
    "    \"curriculum vitae\", \"danse macabre\", \"danse ronde\", \"de facto\", \"de jure\", \"de luxe\", \"de profundis\", \n",
    "    \"de rigeur\", \"de rigueur\", \"de trop\", \"decree nisi\", \"dei gratia\", \"déjà vu\", \"delirium tremens\", \n",
    "    \"demi monde\", \"depending on\", \"deus ex machina\", \"double entendre\", \"double entendres\", \"doubles entendres\", \n",
    "    \"dramatis personae\", \"due to\", \"each other\", \"eau de cologne\", \"eminence grise\", \"en bloc\", \"en famille\", \n",
    "    \"en masse\", \"en passant\", \"en route\", \"en suite\", \"enfant terrible\", \"enfants terribles\", \"entente cordiale\", \n",
    "    \"esprit de corps\", \"et al\", \"et cetera\", \"even if\", \"even so\", \"even though\", \"even when\", \"ever so\", \n",
    "    \"every so often\", \"ex ante\", \"ex army\", \"ex cathedra\", \"ex gratia\", \"ex hypothesi\", \"ex libris\", \"ex officio\", \n",
    "    \"ex parte\", \"ex post\", \"ex post facto\", \"ex silentio\", \"ex tempore\", \"ex turpi causa\", \"ex vitro\", \"ex vivo\", \n",
    "    \"except for\", \"except that\", \"excepting for\", \"fair do's\", \"fait accompli\", \"far from\", \"far off\", \n",
    "    \"faute de mieux\", \"faux ami\", \"faux amis\", \"faux pas\", \"fed up\", \"femme fatale\", \"femmes fatales\", \n",
    "    \"film noir\", \"films noirs\", \"fin de siècle\", \"fines herbes\", \"foie gras\", \"follow up\", \"for certain\", \n",
    "    \"for ever\", \"for example\", \"for fear of\", \"for good\", \"for instance\", \"for keeps\", \"for long\", \"for once\", \n",
    "    \"for sure\", \"for the most part\", \"for the time being\", \"force majeure\", \"from now on\", \"from time to time\", \n",
    "    \"fromage frais\", \"gee whizz\", \"genius loci\", \"getting on for\", \"given that\", \"grand mal\", \"grand prix\", \n",
    "    \"grande dame\", \"grands prix\", \"grown up\", \"grown ups\", \"guardian ad litem\", \"gung ho\", \"habeas corpus\", \n",
    "    \"half way\", \"hara kiri\", \"hard up\", \"hasta la vista\", \"hasta luego\", \"haute couture\", \"haute cuisine\", \n",
    "    \"have nots\", \"heave ho\", \"hey presto\", \"higgledy piggledy\", \"hocus pocus\", \"hoi polloi\",\n",
    "    \"hoity toity\",\"homo sapiens\",\"hooray henry\",\"hooray henrys\",\"hors d'oeuvre\",\"hors d'oeuvres\", \"hors de combat\",\n",
    "    \"hotch potch\",\"hush hush\",\"hysteron proteron\",\"idée fixe\",\"ignis fatuus\",\"in absentia\",\"in accord with\",\"in accordance with\",\n",
    "    \"in addition\",\"in addition to\", \"in aid of\",\"in answer to\",\"in as much as\",\"in association with\",\n",
    "    \"in back of\", \"in between\", \"in brief\",\"in camera\",\"in case\",\"in case of\",\"in charge of\", \"in co-operation with\",\"in common\",\n",
    "    \"in common with\", \"in comparison with\", \"in conjunction with\",\"in connection with\", \"in consultation with\", \"in contact with\", \"in cooperation with\", \"in course with\",\n",
    "    \"in defence of\", \"in defiance of\",\"in excess of\",\"in extremis\",\"in face of\",\"in favor of\",\"in favour of\",\n",
    "    \"in flagrante delicto\",\"in front of\",\"in full\",\"in general\",\"in keeping with\",\"in lieu of\",\"in light of\",\n",
    "    \"in line with\",\"in loco parentis\",\"in medias res\",\"in memoriam\",\"in need of\",\"in order\",\"in part\",\n",
    "    \"in particular\", \"in perpetuum\", \"in place of\", \"in possession of\", \"in private\", \"in proportion to\", \"in propria persona\",\n",
    "    \"in public\",\"in pursuit of\",\"in quest of\",\"in receipt of\",\"in regard to\",\"in relation to\",\"in reply to\",\n",
    "    \"in respect of\",\"in response to\",\"in return for\",\"in search of\",\"in short\",\"in situ\",\"in so far as\",\n",
    "    \"in spite of\",\"in support of\",\"in terms of\",\"in that\",\"in the light of\",\"in the main\",\"in the order of\",\n",
    "    \"in toto\",\"in touch with\",\"in vain\",\"in view of\",\"in vino veritas\",\"in vitro\",\"in vivo\",\"inasmuch as\",\"infra dig\",\n",
    "    \"infra dignitatem\",\"inside out\",\"insofar as\",\"insomuch as\",\n",
    "    \"instead of\", \"inter alia\", \"into line with\", \"ipso facto\", \"irrespective of\", \"je ne sais quoi\",\"joie de vivre\",\n",
    "    \"just about\", \"kind of\", \"know how\", \"kung fu\", \"la dolce vita\", \"laissez faire\", \"lasagne verde\",\"le mot juste\",\"less than\",\n",
    "    \"let 's\", \"let alone\",\"lingua franca\", \"lo and behold\", \"loc cit\", \"locum tenens\", \"locus classicus\",\n",
    "    \"long-term wise\", \"magna carta\", \"magna cum laude\", \"magnum opus\", \"maître d\", \"maître d'\",\"maître d'hôtel\",\n",
    "    \"mal de mer\", \"mardi gras\", \"matter of fact\",\"mea culpa\",\"Médecins sans Frontières\",\n",
    "    \"ménage a trois\",\"mezzo soprano\",  \"modus operandi\", \"modus vivendi\", \"more than\", \"mot juste\",\n",
    "    \"mumbo jumbo\", \"mutatis mutandis\", \"near to\", \"nearer to\",\"nearest to\", \"nem con\", \"next to\",\n",
    "    \"nigh on\",\"nitty gritty\",\"no doubt\",\"no longer\",\n",
    "    \"no matter how\", \"no matter what\", \"no matter when\", \"no matter where\", \n",
    "    \"no matter which\", \"no matter who\", \"no matter whom\", \"no matter whose\", \n",
    "    \"no one\", \"noblesse oblige\", \"nom de guerre\", \"nom de plume\", \"noms de guerre\", \n",
    "    \"noms de plume\", \"non compos mentis\", \"non sequitur\", \"non sequiturs\", \n",
    "    \"none other\", \"none the\", \"none the less\", \"not withstanding\", \"nouveau rich\", \n",
    "    \"nouveau riche\", \"nouveaux riches\", \"nouvelle cuisine\", \"now that\", \"o' course\", \n",
    "    \"obiter dictum\", \"objet d'art\", \"objets d'art\", \"of course\", \"off guard\", \n",
    "    \"off of\", \"oft times\", \"okey doke\", \"okey dokey\", \"old fashioned\", \n",
    "    \"on account of\", \"on behalf of\", \"on board\", \"on the part of\", \"on to\", \n",
    "    \"on top of\", \"once again\", \"once and for all\", \"once more\", \"one 's\", \n",
    "    \"one another\", \"op cit\", \"other than\", \"out front\", \"out of\", \"out of date\", \n",
    "    \"out of line with\", \"out of touch with\", \"outside of\", \"over here\", \"over there\", \n",
    "    \"owing to\", \"papier mâché\", \"par excellence\", \"pari passu\", \"pas de deux\", \n",
    "    \"pâté de foie gras\", \"pax americana\", \"pax britannica\", \"pax romana\", \n",
    "    \"per annum\", \"per capita\", \"per cent\", \"per diem\", \"per se\", \"persona non grata\", \n",
    "    \"personae non gratae\", \"pertaining to\", \"petit bourgeois\", \"petit four\", \n",
    "    \"petit mal\", \"petite bougeoisie\", \"petits bourgeois\", \"pièce de résistance\", \n",
    "    \"pied à terre\", \"pina colada\", \"pina coladas\", \"pince nez\", \"poco a poco\", \n",
    "    \"point blank\", \"porte cochère\", \"post hoc\", \"post meridiem\", \"post mortem\", \n",
    "    \"post mortems\", \"poste restante\", \"pot pourri\", \"prima donna\", \"prima donnas\", \n",
    "    \"prima facie\", \"primus inter pares\", \"prior to\", \"pro forma\", \"pro rata\", \n",
    "    \"pro tem\", \"provided that\", \"providing that\", \"pursuant to\", \"qui vive\", \n",
    "    \"quid pro quo\", \"raison d'être\", \"raisons d'être\", \"reductio ad absurdum\", \n",
    "    \"relative to\", \"rigor mortis\", \"roman à clef\", \"sang froid\", \"save for\", \n",
    "    \"save that\", \"savoir faire\", \"savoir vivre\", \"seeing as\", \"seeing that\", \n",
    "    \"semper fidelis\", \"shin bet\", \"shish kebab\", \"shish kebabs\", \"sine die\", \n",
    "    \"sine qua non\", \"sinn fein\", \"so 's\", \"so as\", \"so called\", \"so long as\", \n",
    "    \"so much as\", \"so that\", \"some one\",\n",
    "    \"son et lumière\", \"sort of\", \"sotto voce\", \"spaghetti bolognese\", \"spina bifida\", \n",
    "    \"spot on\", \"sq feet/metres/etc\", \"sq ft/m/cm/etc.\", \"status quo\", \"straight away\", \n",
    "    \"straight forward\", \"sub judice\", \"sub poena\", \"sub rosa\", \"subject to\", \n",
    "    \"subsequent to\", \"such as\", \"such that\", \"sui generis\", \"sui juris\", \n",
    "    \"summa cum laude\", \"super duper\", \"supposing that\", \"table d'hôte\", \"tabula rasa\", \n",
    "    \"tai chi\", \"tai kwan do\", \"teeny weeny\", \"terra firma\", \"terra incognita\", \n",
    "    \"thanks to\", \"that is\", \"that is to say\", \"through thick and thin\", \"time and again\", \n",
    "    \"time and again\", \"tittle tattle\", \"to and fro\", \"topsy turvy\", \"tour de force\", \n",
    "    \"tours de force\", \"tout court\", \"tout de suite\", \"tutti frutti\", \"ultra vires\", \n",
    "    \"under way\", \"up front\", \"up to\", \"up to date\", \"up to the minute\", \"up until\", \n",
    "    \"upside down\", \"upward of\", \"upwards of\", \"vice versa\", \"vin de table\", \n",
    "    \"vin ordinaire\", \"vis à vis\", \"viva voce\", \"void ab initio\", \"vol au vent\", \n",
    "    \"vols au vent\", \"volte face\", \"vox pop\", \"vox pops\", \"vox populi\", \"well being\", \n",
    "    \"well off\", \"whether or not\", \"wiener schnitzel\", \"wiener schnitzels\", \n",
    "    \"with a view to\", \"with reference to\", \"with regard to\", \"with relation to\", \n",
    "    \"with respect to\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1665b848-95dc-4557-ba79-b9a0166226bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_illegal_chars(text):\n",
    "    return ''.join(c for c in text if c.isprintable() or c in '\\n\\t')\n",
    "\n",
    "def is_polyword(tokens):\n",
    "    phrase = ' '.join([token.text.lower() for token in tokens])\n",
    "    return phrase in polywords\n",
    "\n",
    "def process_text_file(input_txt, output_excel):\n",
    "    with open(input_txt, 'r', encoding='utf-8') as f:\n",
    "        raw_lines = [(i + 1, line.strip()) for i, line in enumerate(f) if line.strip()]\n",
    "    \n",
    "    line_map = {}\n",
    "    for lineno, line in raw_lines:\n",
    "        line_map[lineno] = line\n",
    "\n",
    "    full_text = ' '.join(line for _, line in raw_lines)\n",
    "    doc = nlp(full_text)\n",
    "\n",
    "    rows = []\n",
    "    sent_id = 0\n",
    "    token_id = 0 \n",
    "    current_line_idx = 0\n",
    "\n",
    "    print(\"Processing...\")\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_id += 1  \n",
    "        sentence_tokens = list(sent)  \n",
    "        \n",
    "        polyword_flags = [None] * len(sentence_tokens)  \n",
    "\n",
    "        for token_idx in range(len(sentence_tokens) - 1):  \n",
    "            potential_polyword = sentence_tokens[token_idx:token_idx + 2]\n",
    "            if is_polyword(potential_polyword):\n",
    "                polyword_flags[token_idx] = 'Maybe'\n",
    "                polyword_flags[token_idx + 1] = 'Maybe'\n",
    "\n",
    "        for token_idx, token in enumerate(sentence_tokens):\n",
    "            if token.pos_ == \"SPACE\":\n",
    "                continue  \n",
    "            \n",
    "            polyword_flag = polyword_flags[token_idx] if polyword_flags[token_idx] else ''\n",
    "\n",
    "            try:\n",
    "                token_text = clean_illegal_chars(token.text)\n",
    "                pos = token.pos_\n",
    "    \n",
    "                row = {\n",
    "                    'sent_id': sent_id,\n",
    "                    'token_id': token_id + 1,  \n",
    "                    'token_text': token_text,\n",
    "                    'pos': pos,\n",
    "                    'polyword': polyword_flag,  \n",
    "                    'metaphor': '',\n",
    "                    'motivation': '',\n",
    "                    'comment': ''\n",
    "                }\n",
    "                rows.append(row)\n",
    "                token_id += 1\n",
    "    \n",
    "            except IllegalCharacterError:\n",
    "                print(f\"\\nIllegal character in token '{token.text}' (sentence {sent_id}, token {token_id + 1})\")\n",
    "                print(\"  Likely source lines nearby:\")\n",
    "                for offset in range(-1, 2):\n",
    "                    idx = current_line_idx + offset\n",
    "                    if 0 <= idx < len(raw_lines):\n",
    "                        print(f\"    Line {raw_lines[idx][0]}: {raw_lines[idx][1]}\")\n",
    "                print(\"  Skipping this token.\\n\")\n",
    "                continue\n",
    "\n",
    "        current_line_idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        'sent_id', 'token_id', 'token_text', 'pos',\n",
    "        'polyword', 'metaphor', 'motivation', 'comment'\n",
    "    ])\n",
    "    df.to_excel(output_excel, index=False)\n",
    "\n",
    "# process_text_file('epidemic.txt', 'epidemic.xlsx')\n",
    "# process_text_file('grief_counsellor.txt', 'grief_counsellor.xlsx')\n",
    "# process_text_file('lung_cancer.txt', 'lung_cancer.xlsx')\n",
    "# process_text_file('radiation_therapy.txt', 'radiation_therapy.xlsx')\n",
    "# process_text_file('sniffler.txt', 'sniffler.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f77d7-c831-491d-8388-c3ff29e94fdc",
   "metadata": {},
   "source": [
    "## Inter-annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72e6616a-4ea1-499a-bfe7-5652261e8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grief_counsellor = pd.read_excel(\"annotations/grief_counsellor.xlsx\") \n",
    "radiation_therapy = pd.read_excel('annotations/urte_radiation_therapy.xlsx')\n",
    "immuno1_combined = pd.read_excel('annotations/immuno1_combined.xlsx')\n",
    "epidemic = pd.read_excel('annotations/epidemic.xlsx')\n",
    "lung_cancer = pd.read_excel('annotations/lung_cancer.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06b607d6-62c0-4d8f-aafe-a3759f5aa041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_annotation_agreement(df, annot_cols):\n",
    "    \"\"\"\n",
    "    Computes pairwise Cohen's Kappa scores and Krippendorff's Alpha for given annotation columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe containing annotations.\n",
    "        annot_cols (list of str): The columns with annotations.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with pairwise kappa scores, average kappa, and Krippendorff's alpha.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[annot_cols] = df[annot_cols].replace('_', pd.NA).astype(\"float\")\n",
    "\n",
    "    df_nonempty = df.dropna(subset=annot_cols, how='all')\n",
    "\n",
    "    pairs = list(combinations(annot_cols, 2))\n",
    "    kappas = {}\n",
    "\n",
    "    for a1, a2 in pairs:\n",
    "        pair_df = df_nonempty[[a1, a2]].dropna()\n",
    "        if not pair_df.empty:\n",
    "            score = cohen_kappa_score(pair_df[a1], pair_df[a2])\n",
    "            kappas[f\"{a1} vs {a2}\"] = score\n",
    "\n",
    "    avg_kappa = sum(kappas.values()) / len(kappas) if kappas else None\n",
    "\n",
    "    alpha_data = df_nonempty[annot_cols].T.to_numpy()\n",
    "    alpha = krippendorff.alpha(reliability_data=alpha_data, level_of_measurement='nominal')\n",
    "\n",
    "    print(\"\\nPairwise Cohen's Kappa scores:\")\n",
    "    for pair, score in kappas.items():\n",
    "        print(f\"{pair}: {score:.3f}\")\n",
    "    if avg_kappa is not None:\n",
    "        print(f\"\\nAverage pairwise Cohen’s Kappa: {avg_kappa:.3f}\")\n",
    "    else:\n",
    "        print(\"No valid annotation pairs found.\")\n",
    "    print(f\"\\nKrippendorff’s Alpha: {alpha:.3f}\")\n",
    "\n",
    "    return {\n",
    "        'pairwise_kappas': kappas,\n",
    "        'average_kappa': avg_kappa,\n",
    "        'krippendorff_alpha': alpha\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2832837-5d90-43e2-ad78-26b4c03d9de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Cohen's Kappa scores:\n",
      "urte vs meli: 0.583\n",
      "basti vs meli: 0.629\n",
      "\n",
      "Average pairwise Cohen’s Kappa: 0.606\n",
      "\n",
      "Krippendorff’s Alpha: 0.606\n"
     ]
    }
   ],
   "source": [
    "grief_counsellor_results = compute_annotation_agreement(grief_counsellor, ['urte', 'basti', 'meli'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "151dc618-86ce-4427-9fb0-7955efe8cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Cohen's Kappa scores:\n",
      "Urte vs Meli: 0.597\n",
      "\n",
      "Average pairwise Cohen’s Kappa: 0.597\n",
      "\n",
      "Krippendorff’s Alpha: 0.595\n"
     ]
    }
   ],
   "source": [
    "radiation_therapy_results = compute_annotation_agreement(radiation_therapy, ['Urte', 'Meli'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd99d8a3-567b-4a18-bc30-a48a6105eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Cohen's Kappa scores:\n",
      "Urte vs Melina: 0.606\n",
      "Urte vs Basti: 0.628\n",
      "Melina vs Basti: 0.647\n",
      "\n",
      "Average pairwise Cohen’s Kappa: 0.627\n",
      "\n",
      "Krippendorff’s Alpha: 0.621\n"
     ]
    }
   ],
   "source": [
    "immuno1_combined_results = compute_annotation_agreement(immuno1_combined, ['Urte', 'Melina', 'Basti'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d99f6fe-b3f9-4d24-9bf9-34972f75f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Cohen's Kappa scores:\n",
      "meli vs basti: 0.692\n",
      "\n",
      "Average pairwise Cohen’s Kappa: 0.692\n",
      "\n",
      "Krippendorff’s Alpha: 0.692\n"
     ]
    }
   ],
   "source": [
    "epidemic_results = compute_annotation_agreement(epidemic, ['meli', 'basti'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6181eff-443a-47b2-bebc-5599252cedf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Cohen's Kappa scores:\n",
      "meli vs basti: 0.654\n",
      "\n",
      "Average pairwise Cohen’s Kappa: 0.654\n",
      "\n",
      "Krippendorff’s Alpha: 0.654\n"
     ]
    }
   ],
   "source": [
    "lung_cancer_results = compute_annotation_agreement(lung_cancer, ['meli', 'basti'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55466be4-fbe4-4db6-b3e4-f15974e0e82c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "21b08a77-b71c-4b1d-aacd-ab41b945eec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_9812\\2823135289.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file1 = pd.read_csv(\"i_tried_one_last_time.tsv\", sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FILE 1 SPLITS ---\n",
      "File1 Train: 273 texts, 1917792 tokens\n",
      "File1 Dev: 91 texts, 556296 tokens\n",
      "File1 Test: 91 texts, 755118 tokens\n",
      "\n",
      "--- FILE 2 SPLITS ---\n",
      "File2 Train: 24 texts, 70512 tokens\n",
      "File2 Dev: 8 texts, 16014 tokens\n",
      "File2 Test: 9 texts, 19745 tokens\n",
      "\n",
      "--- COMBINED SPLITS ---\n",
      "Combined Train: 297 texts, 1988304 tokens\n",
      "Combined Dev: 99 texts, 572310 tokens\n",
      "Combined Test: 100 texts, 774863 tokens\n"
     ]
    }
   ],
   "source": [
    "file1 = pd.read_csv(\"i_tried_one_last_time.tsv\", sep=\"\\t\")\n",
    "file2 = pd.read_csv(\"vua_texts_with_mflag.tsv\", sep=\"\\t\")\n",
    "\n",
    "file1['is_it_relevant'] = file1['metaphor'].apply(lambda x: 1 if x == 1 else 0)\n",
    "file1 = file1[['text_id', 'sentence_id', 'token_id', 'token_text', 'metaphor', 'is_it_relevant']]\n",
    "\n",
    "file2 = file2.rename(columns={'sentence_number': 'sentence_id', 'token': 'token_text'})\n",
    "file2['token_id'] = file2.groupby(['text_id', 'sentence_id']).cumcount() + 1\n",
    "file2['is_it_relevant'] = 0  # always 0 for file2\n",
    "file2 = file2[['text_id', 'sentence_id', 'token_id', 'token_text', 'metaphor', 'is_it_relevant']]\n",
    "\n",
    "def split_file_by_text_id(df):\n",
    "    unique_text_ids = df['text_id'].unique()\n",
    "    train_ids, temp_ids = train_test_split(unique_text_ids, test_size=0.4, random_state=42)\n",
    "    dev_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "    \n",
    "    df_train = df[df['text_id'].isin(train_ids)]\n",
    "    df_dev = df[df['text_id'].isin(dev_ids)]\n",
    "    df_test = df[df['text_id'].isin(test_ids)]\n",
    "    \n",
    "    return df_train, df_dev, df_test\n",
    "\n",
    "file1_train, file1_dev, file1_test = split_file_by_text_id(file1)\n",
    "file2_train, file2_dev, file2_test = split_file_by_text_id(file2)\n",
    "\n",
    "train_combined = pd.concat([file1_train, file2_train], ignore_index=True)\n",
    "dev_combined = pd.concat([file1_dev, file2_dev], ignore_index=True)\n",
    "test_combined = pd.concat([file1_test, file2_test], ignore_index=True)\n",
    "\n",
    "for df in [train_combined, dev_combined, test_combined]:\n",
    "    df['text_id'] = df['text_id'].astype(str).str.strip().str.replace('\\ufeff', '').str.replace('\\u200b', '')\n",
    "\n",
    "train_combined.to_csv(\"train_combined.tsv\", sep=\"\\t\", index=False)\n",
    "dev_combined.to_csv(\"dev_combined.tsv\", sep=\"\\t\", index=False)\n",
    "test_combined.to_csv(\"test_combined.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "def print_split_stats(df, name):\n",
    "    num_texts = df['text_id'].nunique()\n",
    "    num_tokens = len(df)\n",
    "    print(f\"{name}: {num_texts} texts, {num_tokens} tokens\")\n",
    "\n",
    "print(\"\\n--- FILE 1 SPLITS ---\")\n",
    "print_split_stats(file1_train, \"File1 Train\")\n",
    "print_split_stats(file1_dev, \"File1 Dev\")\n",
    "print_split_stats(file1_test, \"File1 Test\")\n",
    "\n",
    "print(\"\\n--- FILE 2 SPLITS ---\")\n",
    "print_split_stats(file2_train, \"File2 Train\")\n",
    "print_split_stats(file2_dev, \"File2 Dev\")\n",
    "print_split_stats(file2_test, \"File2 Test\")\n",
    "\n",
    "print(\"\\n--- COMBINED SPLITS ---\")\n",
    "print_split_stats(train_combined, \"Combined Train\")\n",
    "print_split_stats(dev_combined, \"Combined Dev\")\n",
    "print_split_stats(test_combined, \"Combined Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3219447-f324-4c17-aacb-9db660366b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_24724\\4078456299.py:6: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_combined = pd.read_csv(train_final, sep=\"\\t\")\n",
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_24724\\4078456299.py:7: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dev_combined = pd.read_csv(dev_final, sep=\"\\t\")\n",
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_24724\\4078456299.py:8: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_combined = pd.read_csv(test_final, sep=\"\\t\")\n",
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_24724\\4078456299.py:23: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined = pd.read_csv(output_file, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'data_creation/lung_cancer_final.xlsx' as 'extra_004' into 'test.tsv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_24724\\4078456299.py:23: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined = pd.read_csv(output_file, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'data_creation/epidemic_final.xlsx' as 'extra_005' into 'test.tsv'\n"
     ]
    }
   ],
   "source": [
    "train_final = \"train.tsv\"\n",
    "dev_final = \"dev.tsv\"\n",
    "test_final = \"test.tsv\"\n",
    "\n",
    "train_combined = pd.read_csv(train_final, sep=\"\\t\")\n",
    "dev_combined = pd.read_csv(dev_final, sep=\"\\t\")\n",
    "test_combined = pd.read_csv(test_final, sep=\"\\t\")\n",
    "\n",
    "extra_file1 = \"data_creation/grief_counselor_final.xlsx\" \n",
    "extra_file2 = \"data_creation/radiation_therapy_final.xlsx\"\n",
    "extra_file3 = \"data_creation/wonder_cure_final.xlsx\"\n",
    "extra_file4 = \"data_creation/lung_cancer_final.xlsx\"\n",
    "extra_file5 = \"data_creation/epidemic_final.xlsx\"\n",
    "\n",
    "existing_text_ids = set(pd.concat([train_combined, dev_combined, test_combined])['text_id'].unique())\n",
    "\n",
    "def add_extra_file_to_combined(extra_file, output_file, existing_text_ids=None, starting_extra_id=1):\n",
    "    try:\n",
    "        combined = pd.read_csv(output_file, sep=\"\\t\")\n",
    "    except FileNotFoundError:\n",
    "        combined = pd.DataFrame(columns=['text_id', 'sentence_id', 'token_id', 'token_text', 'metaphor', 'is_it_relevant'])\n",
    "\n",
    "    if existing_text_ids is None:\n",
    "        existing_text_ids = set(combined['text_id'].unique())\n",
    "\n",
    "    next_extra_id = starting_extra_id\n",
    "\n",
    "    def generate_unique_text_id(existing_ids):\n",
    "        nonlocal next_extra_id\n",
    "        new_id = f\"extra_{next_extra_id:03}\"\n",
    "        while new_id in existing_ids:\n",
    "            next_extra_id += 1\n",
    "            new_id = f\"extra_{next_extra_id:03}\"\n",
    "        existing_ids.add(new_id)\n",
    "        return new_id\n",
    "\n",
    "    df = pd.read_excel(extra_file)\n",
    "\n",
    "    df_clean = df[['sentence_id', 'token_id', 'token_text', 'FINAL', 'is_it_relevant']].copy()\n",
    "    df_clean = df_clean.rename(columns={'FINAL': 'metaphor'})\n",
    "\n",
    "    df_clean['metaphor'] = df_clean['metaphor'].apply(lambda x: 1 if str(x).strip() == '1' else 0).astype(int)\n",
    "\n",
    "    df_clean['is_it_relevant'] = df_clean['is_it_relevant'].fillna(0).astype(int)\n",
    "\n",
    "    new_text_id = generate_unique_text_id(existing_text_ids)\n",
    "    df_clean['text_id'] = new_text_id\n",
    "\n",
    "    df_clean = df_clean[['text_id', 'sentence_id', 'token_id', 'token_text', 'metaphor', 'is_it_relevant']]\n",
    "\n",
    "    combined = pd.concat([combined, df_clean], ignore_index=True)\n",
    "    combined.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Added '{extra_file}' as '{new_text_id}' into '{output_file}'\")\n",
    "\n",
    "    return combined, existing_text_ids\n",
    "\n",
    "\n",
    "# train_combined, existing_text_ids = add_extra_file_to_combined(extra_file1, train_final, \n",
    "#                                                                existing_text_ids=existing_text_ids, starting_extra_id=1)\n",
    "# train_combined, existing_text_ids = add_extra_file_to_combined(extra_file2, train_final, \n",
    "#                                                                existing_text_ids=existing_text_ids, starting_extra_id=1)\n",
    "# test_combined, existing_text_ids = add_extra_file_to_combined(extra_file3, test_final, \n",
    "#                                                                existing_text_ids=existing_text_ids, starting_extra_id=1)\n",
    "test_combined, existing_text_ids = add_extra_file_to_combined(extra_file4, test_final, \n",
    "                                                               existing_text_ids=existing_text_ids, starting_extra_id=1)\n",
    "test_combined, existing_text_ids = add_extra_file_to_combined(extra_file5, test_final, \n",
    "                                                               existing_text_ids=existing_text_ids, starting_extra_id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b3f37-6a75-4706-8eaf-158b76c4a722",
   "metadata": {},
   "source": [
    "## Full-text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ee76ec-baec-485c-a503-9b2a8ad8d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '../data/20221220 Metaforen IT_DEF.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7867848-f495-4791-a8a7-7571a895e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(a, b, threshold=85):\n",
    "    return ratio(a, b) >= threshold\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return ''.join(text.split())\n",
    "\n",
    "@lru_cache(maxsize=512)\n",
    "def find_full_text(fragment, txt_folders):\n",
    "    \"\"\"\n",
    "    Finds a full text containing the given fragment, ignoring whitespace differences.\n",
    "    \"\"\"\n",
    "    fragment_clean = remove_whitespace(fragment.strip())\n",
    "    if not fragment_clean:\n",
    "        return None\n",
    "\n",
    "    for folder in txt_folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.txt'):\n",
    "                filepath = os.path.join(folder, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    full_text = f.read()\n",
    "                    full_text_clean = remove_whitespace(full_text)\n",
    "\n",
    "                    if fragment_clean in full_text_clean:\n",
    "                        return full_text\n",
    "\n",
    "                    words = full_text.split()\n",
    "                    frag_words = fragment.split()\n",
    "                    window_size = len(frag_words)\n",
    "\n",
    "                    for i in range(len(words) - window_size + 1):\n",
    "                        window = ' '.join(words[i:i+window_size])\n",
    "                        window_clean = remove_whitespace(window)\n",
    "\n",
    "                        if abs(len(window_clean) - len(fragment_clean)) > 20:\n",
    "                            continue\n",
    "\n",
    "                        if is_similar(fragment_clean, window_clean):\n",
    "                            return full_text\n",
    "    return None\n",
    "\n",
    "\n",
    "def preprocess_excel_and_texts(input_excel, txt_folders, output_tsv):\n",
    "    df = pd.read_excel(input_excel)\n",
    "\n",
    "    rows = []\n",
    "    token_id = 1  \n",
    "\n",
    "    for text_num, row in tqdm(enumerate(df.itertuples(index=False), start=1), total=len(df), desc=\"Processing texts\"):\n",
    "        fragment = str(row.Text1).strip() if pd.notna(row.Text1) else ''\n",
    "        metaphor_word = str(row.Metafoor).strip().lower() if pd.notna(row.Metafoor) else None\n",
    "\n",
    "        full_text = find_full_text(fragment, tuple(txt_folders))\n",
    "        if full_text is None:\n",
    "            print(f\"Warning: Fragment not found for text {text_num}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        doc = nlp(full_text)\n",
    "\n",
    "        for sent_id, sent in enumerate(doc.sents, start=1):\n",
    "            for token in sent:\n",
    "                if token.is_space:\n",
    "                    continue\n",
    "\n",
    "                token_text = token.text\n",
    "                is_metaphor = 1 if metaphor_word and token_text.lower() == metaphor_word else 0\n",
    "\n",
    "                rows.append({\n",
    "                    'text_id': text_num,\n",
    "                    'sentence_id': sent_id,\n",
    "                    'token_id': token_id,\n",
    "                    'token_text': token_text,\n",
    "                    'metaphor': is_metaphor,\n",
    "                    'Categorie': row.Categorie if is_metaphor else '',\n",
    "                    'Domein1': row.Domein1 if is_metaphor else '',\n",
    "                    'Domein2': row.Domein2 if is_metaphor else ''\n",
    "                })\n",
    "                token_id += 1\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    df_out.to_csv(output_tsv, sep='\\t', index=False)\n",
    "    print(f\"Preprocessing complete! Saved to {output_tsv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bfcc0-b385-46d2-b760-071019c8110c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt_folders = ['../data/Corpus_IT_UK_news/Corpus_IT_UK_news', '../data/Dataset wetenschappelijke artikelen txt/txt']\n",
    "preprocess_excel_and_texts('../data/20221220 Metaforen IT_DEF.xlsx', txt_folders, 'full_texts_recall.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f671474-db5d-49d2-83d0-62440c67280c",
   "metadata": {},
   "source": [
    "## Sentence-level datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "230298ce-6622-4a39-87fe-58977b884fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens_with_hyphen_handling(tokens):\n",
    "    merged_tokens = []\n",
    "    merged_indices = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        if i + 1 < len(tokens) and token.endswith('-') and len(token) > 1:\n",
    "            next_token = tokens[i + 1]\n",
    "            if next_token.isalpha() and next_token[0].islower():\n",
    "                merged_tokens.append(token[:-1] + next_token)\n",
    "                merged_indices.append([i, i + 1])\n",
    "                i += 2\n",
    "                continue\n",
    "        merged_tokens.append(token)\n",
    "        merged_indices.append([i])\n",
    "        i += 1\n",
    "    return merged_tokens, merged_indices\n",
    "\n",
    "def clean_token_text(token):\n",
    "    token = token.replace('\\u201c', '\"').replace('\\u201d', '\"')  \n",
    "    token = token.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  \n",
    "\n",
    "    token = re.sub(r'[^\\w\\s\\.\\,\\:\\;\\-\\(\\)\\'\\\"\\/\\[\\]\\{\\}\\!\\?\\u00B0]', '', token, flags=re.UNICODE)\n",
    "\n",
    "    token = re.sub(r'[\\u00A0\\u200B\\u200C\\u200D\\uFEFF]', '', token)\n",
    "\n",
    "    return token\n",
    "\n",
    "def preprocess_sentences(row, text_num, token_id_start):\n",
    "    rows = []\n",
    "    text1 = str(row.Text1)\n",
    "    text2 = str(row.Text2)\n",
    "\n",
    "    sent1 = list(nlp(text1).sents)[-1].text if list(nlp(text1).sents) else ''\n",
    "    sent2 = list(nlp(text2).sents)[0].text if list(nlp(text2).sents) else ''\n",
    "    full_sent = sent1 + \" \" + sent2\n",
    "    doc = nlp(full_sent)\n",
    "\n",
    "    tokens = [t.text for t in doc if not t.is_space]\n",
    "    norm_tokens, norm_to_orig = normalize_tokens_with_hyphen_handling(tokens)\n",
    "\n",
    "    norm_tokens = [clean_token_text(t) for t in norm_tokens]\n",
    "\n",
    "    metaphors_raw = str(row.Metafoor)\n",
    "    metaphors = [m.strip().lower() for m in metaphors_raw.split(',') if m.strip()]\n",
    "\n",
    "    if not any(\n",
    "        any(metaphor in nt.lower() for nt in norm_tokens)\n",
    "        for metaphor in metaphors\n",
    "    ):\n",
    "        print(f\"Metaphor(s) '{', '.join(metaphors)}' not found in row {text_num}. Skipping.\")\n",
    "        return rows, token_id_start\n",
    "\n",
    "    metaphor_token_indices = set()\n",
    "    for metaphor in metaphors:\n",
    "        metaphor_parts = metaphor.split()\n",
    "        for idx in range(len(norm_tokens) - len(metaphor_parts) + 1):\n",
    "            window = norm_tokens[idx:idx + len(metaphor_parts)]\n",
    "            if [t.lower() for t in window] == metaphor_parts:\n",
    "                for offset in range(len(metaphor_parts)):\n",
    "                    metaphor_token_indices.add(idx + offset)\n",
    "\n",
    "    token_id = token_id_start\n",
    "    sentence_id = 1\n",
    "    for idx, norm_token in enumerate(norm_tokens):\n",
    "        token_text = ' '.join(tokens[i] for i in norm_to_orig[idx])\n",
    "        token_text = clean_token_text(token_text) \n",
    "        is_metaphor = 1 if idx in metaphor_token_indices else 0\n",
    "        rows.append({\n",
    "            'text_id': text_num,\n",
    "            'sentence_id': sentence_id,\n",
    "            'token_id': token_id,\n",
    "            'token_text': token_text,\n",
    "            'metaphor': is_metaphor,\n",
    "            'Categorie': row.Categorie if is_metaphor else '',\n",
    "            'Domein1': row.Domein1 if is_metaphor else '',\n",
    "            'Domein2': row.Domein2 if is_metaphor else ''\n",
    "        })\n",
    "        token_id += 1\n",
    "\n",
    "    return rows, token_id\n",
    "\n",
    "def process_excel(input_excel, output_tsv):\n",
    "    df = pd.read_excel(input_excel)\n",
    "    all_rows = []\n",
    "    token_id = 1\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        rows, token_id = preprocess_sentences(row, idx + 1, token_id)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    out_df = pd.DataFrame(all_rows)\n",
    "    out_df.to_csv(output_tsv, sep='\\t', index=False)\n",
    "    print(f\"Saved processed file to {output_tsv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f9015-4954-43f1-a323-4841bf07f209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_excel('../data/20221220 Metaforen IT_DEF.xlsx', 'sentences_immuno.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e0a23-3793-4018-9125-c23b98eb5821",
   "metadata": {},
   "source": [
    "## Find sentences with metaphors in VU corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5149d39-584a-40ae-a64a-df6604d64fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences WITH mflag: 120\n",
      "Sentences WITHOUT mflag selected (limit 90): 90\n",
      "Total direct metaphors marked (mrw + type=met) in sentences without mflag: 90\n",
      "[DONE] Saved 120 sentences WITH mflag to metaphors_with_mflag.tsv\n",
      "[DONE] Saved 90 sentences WITHOUT mflag to metaphors_without_mflag.tsv\n"
     ]
    }
   ],
   "source": [
    "def extract_metaphor_sentences(xml_path, output_tsv_with_mflag, output_tsv_without_mflag, direct_metaphor_limit=40):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    namespace = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "    \n",
    "    texts = root.findall('.//tei:text', namespace)\n",
    "    \n",
    "    sentences_with_mflag = []\n",
    "    sentences_without_mflag = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text_id = text.attrib.get('{http://www.w3.org/XML/1998/namespace}id', '').strip()\n",
    "        if not text_id:\n",
    "            continue\n",
    "        \n",
    "        for s in text.findall('.//tei:s', namespace):\n",
    "            sentence_number = s.attrib.get('n', '').strip()\n",
    "            tokens = []\n",
    "            has_mflag = False\n",
    "            \n",
    "            for word in s.findall('tei:w', namespace):\n",
    "                seg_els = word.findall('tei:seg', namespace)\n",
    "                \n",
    "                token_text = None\n",
    "                for seg_el in seg_els:\n",
    "                    if seg_el.text and seg_el.text.strip():\n",
    "                        token_text = seg_el.text.strip()\n",
    "                        break\n",
    "                if not token_text:\n",
    "                    token_text = word.text.strip() if word.text else ''\n",
    "                \n",
    "                lemma = word.attrib.get('lemma', '').strip()\n",
    "                \n",
    "                mflag = 0\n",
    "                for seg in seg_els:\n",
    "                    if seg.attrib.get('function') == 'mFlag':\n",
    "                        mflag = 1\n",
    "                        has_mflag = True\n",
    "                \n",
    "                tokens.append({\n",
    "                    'text_id': text_id,\n",
    "                    'sentence_number': sentence_number,\n",
    "                    'token_text': token_text,\n",
    "                    'lemma': lemma,\n",
    "                    'mflag': mflag,\n",
    "                    'seg_els': seg_els\n",
    "                })\n",
    "            \n",
    "            if has_mflag:\n",
    "                sentences_with_mflag.append(tokens)\n",
    "            else:\n",
    "                sentences_without_mflag.append(tokens)\n",
    "    \n",
    "    for tokens in sentences_with_mflag:\n",
    "        for token in tokens:\n",
    "            token['metaphor'] = 0\n",
    "            for seg in token['seg_els']:\n",
    "                if seg.attrib.get('function') == 'mrw' and seg.attrib.get('type') == 'lit':\n",
    "                    token['metaphor'] = 1\n",
    "                    break\n",
    "            del token['seg_els']\n",
    "    \n",
    "    selected_sentences_without_mflag = []\n",
    "    metaphor_sentence_count = 0\n",
    "    \n",
    "    for tokens in sentences_without_mflag:\n",
    "        metaphor_candidates = []\n",
    "        for idx, token in enumerate(tokens):\n",
    "            token['metaphor'] = 0\n",
    "            for seg in token['seg_els']:\n",
    "                if seg.attrib.get('function') == 'mrw' and seg.attrib.get('type') == 'met':\n",
    "                    metaphor_candidates.append(idx)\n",
    "        \n",
    "        if metaphor_candidates:\n",
    "            if metaphor_sentence_count < direct_metaphor_limit:\n",
    "                chosen_idx = random.choice(metaphor_candidates)\n",
    "                tokens[chosen_idx]['metaphor'] = 1\n",
    "                selected_sentences_without_mflag.append(tokens)\n",
    "                metaphor_sentence_count += 1\n",
    "        \n",
    "        for token in tokens:\n",
    "            if 'seg_els' in token:\n",
    "                del token['seg_els']\n",
    "    \n",
    "    print(f\"Sentences WITH mflag: {len(sentences_with_mflag)}\")\n",
    "    print(f\"Sentences WITHOUT mflag selected (limit {direct_metaphor_limit}): {len(selected_sentences_without_mflag)}\")\n",
    "    print(f\"Total direct metaphors marked (mrw + type=met) in sentences without mflag: {metaphor_sentence_count}\")\n",
    "    \n",
    "    rows_with = []\n",
    "    sentence_id = 0\n",
    "    for sent_tokens in sentences_with_mflag:\n",
    "        sentence_id += 1\n",
    "        for token in sent_tokens:\n",
    "            rows_with.append({\n",
    "                'sentence_id': sentence_id,\n",
    "                'sentence_number': token['sentence_number'],\n",
    "                'token_text': token['token_text'],\n",
    "                'lemma': token['lemma'],\n",
    "                'metaphor': token['metaphor'],\n",
    "                'mflag': token['mflag']\n",
    "            })\n",
    "    df_with = pd.DataFrame(rows_with)\n",
    "    df_with.to_csv(output_tsv_with_mflag, sep='\\t', index=False)\n",
    "    \n",
    "    rows_without = []\n",
    "    sentence_id = 0\n",
    "    for sent_tokens in selected_sentences_without_mflag:\n",
    "        sentence_id += 1\n",
    "        for token in sent_tokens:\n",
    "            rows_without.append({\n",
    "                'sentence_id': sentence_id,\n",
    "                'sentence_number': token['sentence_number'],\n",
    "                'token_text': token['token_text'],\n",
    "                'lemma': token['lemma'],\n",
    "                'metaphor': token['metaphor'],\n",
    "                'mflag': token['mflag']\n",
    "            })\n",
    "    df_without = pd.DataFrame(rows_without)\n",
    "    df_without.to_csv(output_tsv_without_mflag, sep='\\t', index=False)\n",
    "    \n",
    "    print(f'[DONE] Saved {len(sentences_with_mflag)} sentences WITH mflag to {output_tsv_with_mflag}')\n",
    "    print(f'[DONE] Saved {len(selected_sentences_without_mflag)} sentences WITHOUT mflag to {output_tsv_without_mflag}')\n",
    "\n",
    "extract_metaphor_sentences(\n",
    "    '../data/vu dataset/VUAMC.xml',\n",
    "    'metaphors_with_mflag.tsv',\n",
    "    'metaphors_without_mflag.tsv',\n",
    "    direct_metaphor_limit=90\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf7467-64f4-42e5-bbcf-dd5d73192690",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de691d8c-f55c-4864-904c-8f4e18bfe0e0",
   "metadata": {},
   "source": [
    "### Creation of train, dev, test splits of sentence level files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e1c882b9-a7bd-40c3-a314-74dcb36f07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_small = pd.read_csv(\"sentences_immuno.tsv\", sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4bffd0f5-fab4-4273-8b5a-2a71903793af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text_id', 'sentence_id', 'token_id', 'token_text', 'metaphor',\n",
      "       'Categorie', 'Domein1', 'Domein2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_train_small.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ba89f0e-a695-448e-90b6-c4944debb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_text_ids = df_train_small['text_id'].unique().tolist()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(unique_text_ids)\n",
    "\n",
    "n_total = len(unique_text_ids)\n",
    "n_train = int(n_total * 0.6)\n",
    "n_dev = int(n_total * 0.2)\n",
    "n_test = n_total - n_train - n_dev \n",
    "\n",
    "train_ids = unique_text_ids[:n_train]\n",
    "dev_ids = unique_text_ids[n_train:n_train + n_dev]\n",
    "test_ids = unique_text_ids[n_train + n_dev:]\n",
    "\n",
    "train_small = df_train_small[df_train_small['text_id'].isin(train_ids)]\n",
    "dev_small = df_train_small[df_train_small['text_id'].isin(dev_ids)]\n",
    "test_small = df_train_small[df_train_small['text_id'].isin(test_ids)]\n",
    "\n",
    "train_small.to_csv(\"train_small.tsv\", sep=\"\\t\", index=False)\n",
    "dev_small.to_csv(\"dev_small.tsv\", sep=\"\\t\", index=False)\n",
    "test_small.to_csv(\"test_small.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d31114c6-8d94-4e26-b281-5556fa0736d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Updated datasets saved: 11189 train, 5316 dev, 4967 test tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_34748\\2233889307.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_small['is_it_relevant'] = (test_small['metaphor'] == 1).astype(int)\n"
     ]
    }
   ],
   "source": [
    "train_small = pd.read_csv(\"train_small.tsv\", sep=\"\\t\")\n",
    "dev_small = pd.read_csv(\"dev_small.tsv\", sep=\"\\t\")\n",
    "test_small = pd.read_csv(\"test_small.tsv\", sep=\"\\t\")\n",
    "\n",
    "for df in [train_small, dev_small, test_small]:\n",
    "    if 'sentence_id' in df.columns:\n",
    "        df.rename(columns={'sentence_id': 'sentence_number'}, inplace=True)\n",
    "\n",
    "keep_cols = ['text_id', 'sentence_number', 'token_text', 'metaphor']\n",
    "train_small = train_small[keep_cols]\n",
    "dev_small = dev_small[keep_cols]\n",
    "test_small = test_small[keep_cols]\n",
    "\n",
    "train_small['is_it_relevant'] = (train_small['metaphor'] == 1).astype(int)\n",
    "dev_small['is_it_relevant'] = (dev_small['metaphor'] == 1).astype(int)\n",
    "test_small['is_it_relevant'] = (test_small['metaphor'] == 1).astype(int)\n",
    "\n",
    "mflag_df = pd.read_csv(\"metaphors_with_mflag.tsv\", sep=\"\\t\")\n",
    "nomflag_df = pd.read_csv(\"metaphors_without_mflag.tsv\", sep=\"\\t\")\n",
    "\n",
    "mflag_df.rename(columns={'sentence_id': 'sentence_number'}, inplace=True)\n",
    "nomflag_df.rename(columns={'sentence_id': 'sentence_number'}, inplace=True)\n",
    "\n",
    "mflag_df = mflag_df[keep_cols]\n",
    "nomflag_df = nomflag_df[keep_cols]\n",
    "\n",
    "mflag_df['is_it_relevant'] = 0\n",
    "nomflag_df['is_it_relevant'] = 0\n",
    "\n",
    "max_text_id = max(\n",
    "    train_small['text_id'].max(),\n",
    "    dev_small['text_id'].max(),\n",
    "    test_small['text_id'].max()\n",
    ")\n",
    "max_sentence_number = max(\n",
    "    train_small['sentence_number'].max(),\n",
    "    dev_small['sentence_number'].max(),\n",
    "    test_small['sentence_number'].max()\n",
    ")\n",
    "\n",
    "mflag_df['text_id'] += max_text_id + 1\n",
    "mflag_df['sentence_number'] += max_sentence_number + 1\n",
    "\n",
    "nomflag_df['text_id'] += max_text_id + 10001\n",
    "nomflag_df['sentence_number'] += max_sentence_number + 10001\n",
    "\n",
    "def split_by_sentence(df, proportions, seed=42):\n",
    "    random.seed(seed)\n",
    "    sentence_ids = df['sentence_number'].unique().tolist()\n",
    "    random.shuffle(sentence_ids)\n",
    "\n",
    "    n = len(sentence_ids)\n",
    "    n_train = int(n * proportions[0])\n",
    "    n_dev = int(n * proportions[1])\n",
    "\n",
    "    train_ids = sentence_ids[:n_train]\n",
    "    dev_ids = sentence_ids[n_train:n_train + n_dev]\n",
    "    test_ids = sentence_ids[n_train + n_dev:]\n",
    "\n",
    "    return (\n",
    "        df[df['sentence_number'].isin(train_ids)],\n",
    "        df[df['sentence_number'].isin(dev_ids)],\n",
    "        df[df['sentence_number'].isin(test_ids)]\n",
    "    )\n",
    "\n",
    "total_sentences = sum([\n",
    "    train_small['sentence_number'].nunique(),\n",
    "    dev_small['sentence_number'].nunique(),\n",
    "    test_small['sentence_number'].nunique()\n",
    "])\n",
    "\n",
    "proportions = [\n",
    "    train_small['sentence_number'].nunique() / total_sentences,\n",
    "    dev_small['sentence_number'].nunique() / total_sentences\n",
    "]\n",
    "\n",
    "mflag_train, mflag_dev, mflag_test = split_by_sentence(mflag_df, proportions)\n",
    "nomflag_train, nomflag_dev, nomflag_test = split_by_sentence(nomflag_df, proportions)\n",
    "\n",
    "train_all = pd.concat([train_small, mflag_train, nomflag_train])\n",
    "dev_all = pd.concat([dev_small, mflag_dev, nomflag_dev])\n",
    "test_all = pd.concat([test_small, mflag_test, nomflag_test])\n",
    "\n",
    "train_all.to_csv(\"train_small_updated.tsv\", sep=\"\\t\", index=False)\n",
    "dev_all.to_csv(\"dev_small_updated.tsv\", sep=\"\\t\", index=False)\n",
    "test_all.to_csv(\"test_small_updated.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"[DONE] Updated datasets saved: {len(train_all)} train, {len(dev_all)} dev, {len(test_all)} test tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bcc502-1b2b-441e-a820-a954491d54c8",
   "metadata": {},
   "source": [
    "### Duplicating extra files to add to sentence-level datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "87cd2a05-213a-4ed3-a1b9-af31048cdb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_creation/grief_counselor_final.xlsx...\n",
      "Processing data_creation/radiation_therapy_final.xlsx...\n",
      "Processing data_creation/wonder_cure_final.xlsx...\n",
      "Processing data_creation/epidemic_final.xlsx...\n",
      "Processing data_creation/lung_cancer_final.xlsx...\n",
      "\n",
      "Saved duplicated sentences with metaphors to 'extra_sentences.tsv'\n"
     ]
    }
   ],
   "source": [
    "def clean_token_text(token):\n",
    "    if not isinstance(token, str):\n",
    "        return ''\n",
    "    \n",
    "    token = token.replace('\\u201c', '\"').replace('\\u201d', '\"')  \n",
    "    token = token.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  \n",
    "    \n",
    "    try:\n",
    "        token = token.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    token = ''.join(c for c in token if c.isprintable() or c in '\\n\\t')\n",
    "    \n",
    "    return token\n",
    "\n",
    "\n",
    "extra_files = [\n",
    "    \"data_creation/grief_counselor_final.xlsx\",\n",
    "    \"data_creation/radiation_therapy_final.xlsx\",\n",
    "    \"data_creation/wonder_cure_final.xlsx\",\n",
    "    \"data_creation/epidemic_final.xlsx\",\n",
    "    \"data_creation/lung_cancer_final.xlsx\"\n",
    "]\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "for file in extra_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    df = pd.read_excel(file)\n",
    "\n",
    "    df['FINAL'] = df['FINAL'].fillna(0).astype(int)\n",
    "    if 'is_it_relevant' not in df.columns:\n",
    "        df['is_it_relevant'] = df['FINAL']\n",
    "    else:\n",
    "        df['is_it_relevant'] = df['is_it_relevant'].fillna(0).astype(int)\n",
    "\n",
    "    df['token_text'] = df['token_text'].apply(clean_token_text)\n",
    "    df = df.rename(columns={'FINAL': 'metaphor'})\n",
    "\n",
    "    for sent_id, group in df.groupby(\"sentence_id\"):\n",
    "        group = group.sort_values(\"token_id\").copy().reset_index(drop=True)\n",
    "\n",
    "        metaphor_spans = []\n",
    "        current_span = []\n",
    "\n",
    "        for i, row in group.iterrows():\n",
    "            if row[\"metaphor\"] == 1:\n",
    "                current_span.append(i)\n",
    "            elif current_span:\n",
    "                metaphor_spans.append(current_span)\n",
    "                current_span = []\n",
    "        if current_span:\n",
    "            metaphor_spans.append(current_span)\n",
    "\n",
    "        if not metaphor_spans:\n",
    "            continue  \n",
    "\n",
    "        for idx, span in enumerate(metaphor_spans):\n",
    "            duplicated = group.copy()\n",
    "\n",
    "            duplicated[\"metaphor\"] = 0\n",
    "            duplicated[\"is_it_relevant\"] = 0\n",
    "\n",
    "            duplicated.loc[span, \"metaphor\"] = 1\n",
    "            duplicated.loc[span, \"is_it_relevant\"] = group.loc[span, \"is_it_relevant\"]\n",
    "\n",
    "            duplicated[\"text_id\"] = f\"extra_{file.split('/')[-1].split('_')[0]}_{sent_id}_{idx:02}\"\n",
    "            output_rows.append(duplicated)\n",
    "\n",
    "if output_rows:\n",
    "    final_df = pd.concat(output_rows, ignore_index=True)\n",
    "    final_df = final_df[[\"text_id\", \"sentence_id\", \"token_text\", \"pos\", \"metaphor\", \"is_it_relevant\"]]\n",
    "    final_df.to_csv(\"extra_sentences.tsv\", sep=\"\\t\", index=False)\n",
    "    print(f\"\\nSaved duplicated sentences with metaphors to 'extra_sentences.tsv'\")\n",
    "else:\n",
    "    print(\"No metaphor spans found in any file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f5e9e-9e00-40c0-9917-c780e1f655e9",
   "metadata": {},
   "source": [
    "### Creation of train, dev, test splits of complete text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77645bea-0050-49ee-8719-01feddee1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melou\\AppData\\Local\\Temp\\ipykernel_26712\\1775721251.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(\"i_tried_one_more_time.tsv\", sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"full_texts_recall.tsv\", sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06c82950-04c3-4645-8dcc-5c12e278131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_text_ids = df_train['text_id'].unique().tolist()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(unique_text_ids)\n",
    "\n",
    "n_total = len(unique_text_ids)\n",
    "n_train = int(n_total * 0.6)\n",
    "n_dev = int(n_total * 0.2)\n",
    "n_test = n_total - n_train - n_dev \n",
    "\n",
    "train_ids = unique_text_ids[:n_train]\n",
    "dev_ids = unique_text_ids[n_train:n_train + n_dev]\n",
    "test_ids = unique_text_ids[n_train + n_dev:]\n",
    "\n",
    "train = df_train[df_train['text_id'].isin(train_ids)]\n",
    "dev = df_train[df_train['text_id'].isin(dev_ids)]\n",
    "test = df_train[df_train['text_id'].isin(test_ids)]\n",
    "\n",
    "train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "dev.to_csv(\"dev.tsv\", sep=\"\\t\", index=False)\n",
    "test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b6256-43a0-4fb5-82f2-5f0d49aad1a4",
   "metadata": {},
   "source": [
    "### Combined sentence-level datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77039b3a-c557-4d92-86c5-3474d8b05b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../train_small_updated.tsv'\n",
    "dev_file = '../dev_small_updated.tsv'\n",
    "test_file = '../test_small_updated.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c089a8-0299-4536-a23b-6f0d6804ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_file = '../test_sentences_anno.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8217c4b5-9cc4-49ad-be0c-35b04c6b4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_file, sep=\"\\t\", header=0)\n",
    "dev = pd.read_csv(dev_file, sep=\"\\t\", header=0)\n",
    "test = pd.read_csv(test_file, sep=\"\\t\", header=0)\n",
    "anno = pd.read_excel(anno_file, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4ab357-797b-496d-b816-28b9190f575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "        \"text_id\",\n",
    "        \"sentence_number\",\n",
    "        \"token_text\",\n",
    "        \"metaphor\",\n",
    "        \"is_it_relevant\",\n",
    "    ]\n",
    "\n",
    "dev_df = pd.read_csv(\n",
    "        dev_file,\n",
    "        sep=\"\\t\",\n",
    "        names=column_names,\n",
    "        encoding=\"utf-8\",\n",
    "        header=0,\n",
    "        na_filter=False, \n",
    "    )\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "        train_file,\n",
    "        sep=\"\\t\",\n",
    "        names=column_names,\n",
    "        encoding=\"utf-8\",\n",
    "        header=0,\n",
    "        na_filter=False,\n",
    "    )\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "        test_file,\n",
    "        sep=\"\\t\",\n",
    "        names=column_names,\n",
    "        encoding=\"utf-8\",\n",
    "        header=0,\n",
    "        na_filter=False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f763b2-d1ba-4817-94a5-c3abc5e5cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_token_level_to_sentence_level(tsv_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Convert token-level metaphor annotations into sentence-level,\n",
    "    duplicating the sentence for each metaphor token that passes POS filtering.\n",
    "\n",
    "    Parameters:\n",
    "    - tsv_path: path to input TSV\n",
    "    - output_path: path to save output TSV (optional)\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame with columns ['text_id' (if available), 'sentence_id', 'sentence_tokens',\n",
    "      'target_token_index', 'is_metaphor', 'is_it_relevant', 'pos']\n",
    "    \"\"\"\n",
    "\n",
    "    _, ext = os.path.splitext(tsv_path)\n",
    "    if ext.lower() in ['.xls', '.xlsx']:\n",
    "        df = pd.read_excel(tsv_path)\n",
    "    elif ext.lower() in ['.tsv', '.txt']:\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format.\")\n",
    "\n",
    "    sentence_col = 'sentence_number' if 'sentence_number' in df.columns else 'sentence_id'\n",
    "    metaphor_col = 'metaphor' if 'metaphor' in df.columns else 'FINAL'\n",
    "    text_id_available = 'text_id' in df.columns\n",
    "\n",
    "    required_columns = {sentence_col, metaphor_col, 'token_text', 'is_it_relevant'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        missing = required_columns - set(df.columns)\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    group_cols = ['text_id', sentence_col] if text_id_available else [sentence_col]\n",
    "\n",
    "    sentence_level_data = []\n",
    "    grouped = df.groupby(group_cols, sort=False)\n",
    "\n",
    "    for group_keys, group in grouped:\n",
    "        tokens = [\n",
    "            token if pd.notna(token) and str(token).strip() != '' else '[empty]'\n",
    "            for token in group['token_text'].tolist()\n",
    "        ]\n",
    "        metaphors = group[metaphor_col].tolist()\n",
    "        relevances = group['is_it_relevant'].tolist()\n",
    "\n",
    "        for i, is_metaphor in enumerate(metaphors):\n",
    "            if is_metaphor == 1:\n",
    "                sentence_info = {\n",
    "                    sentence_col: group[sentence_col].iloc[0],\n",
    "                    'sentence_tokens': tokens,\n",
    "                    'target_token_index': i,\n",
    "                    'is_metaphor': 1,\n",
    "                    'is_it_relevant': int(relevances[i]),\n",
    "                    # 'pos': pos_tag,\n",
    "                }\n",
    "                if text_id_available:\n",
    "                    sentence_info['text_id'] = group['text_id'].iloc[0]\n",
    "\n",
    "                sentence_level_data.append(sentence_info)\n",
    "\n",
    "    result_df = pd.DataFrame(sentence_level_data)\n",
    "\n",
    "    if output_path is not None:\n",
    "        result_df.to_csv(output_path, sep='\\t', index=False)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "test_df_sentence = convert_token_level_to_sentence_level(test_file, 'test_sentences.tsv')\n",
    "train_df_sentence = convert_token_level_to_sentence_level(train_file, 'train_sentences.tsv')\n",
    "dev_df_sentence = convert_token_level_to_sentence_level(dev_file, 'dev_sentences.tsv')\n",
    "anno_df_sentence = convert_token_level_to_sentence_level(anno_file, 'anno_sentences.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
